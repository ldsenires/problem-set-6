---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Luis Se√±ires"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*LS\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*LS\*\* (2 point)
3. Late coins used this pset: \*\*00\*\* Late coins left after submission: \*\*01\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 
from zipfile import ZipFile
import os
import re

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
# Extract data from zip file
base = r"C:\Users\LUIS\Documents\GitHub\problem-set-6"
folder = "waze_data"
path = os.path.join(base, folder, "waze_data_sample.csv")

with ZipFile("waze_data.zip", "r") as f:
    f.extractall(folder)

# Load sample data
df_waze_sample = pd.read_csv(path)
```

```{python}
# Get variable names and data types
df_cols = pd.DataFrame()
df_cols["name"] = df_waze_sample.columns.tolist()
df_cols["dtype"] = df_waze_sample.dtypes.tolist()

# Assign data types using Altair syntax
df_cols["altair_dtype"] = ["O", "N", "Q", "Q", "N", "N",
                            "N", "N", "N", "O", "Q", "Q", "O", "-", "-", "-"]

# Print table
print(df_cols.to_markdown())
```

2. 

```{python}
# Load full data
folder = "waze_data"
path = os.path.join(base, folder, "waze_data.csv")

df_waze = pd.read_csv(path)

# Count number of missing and non-missing values
na_count = df_waze.isna().sum()
non_na_count = df_waze.notna().sum()

# Create new dataframe with counts
df_count = pd.DataFrame({
    "name": na_count.index,
    "na_count": na_count.values,
    "non_na_count": non_na_count.values
})

# Melt data for stacking
df_count_long = df_count.melt(id_vars="name", value_vars=[
                              "na_count", "non_na_count"], var_name="count_type", value_name="count")

# Create stacked bar chart
chart_stacked_count = alt.Chart(df_count_long).mark_bar().encode(
    alt.X("name:N"),
    alt.Y("sum(count):Q"),
    alt.Color("count_type")
)

chart_stacked_count
```

The variables with missing values are nThumbsUp, street, and subtype, with nThumbsUp having the highest share of missing values.

3. 

```{python}
# Combine data by "type" and "subtype"
df_type = df_waze.groupby(
    ["type", "subtype"], dropna=False).size().reset_index()

# Get unique values
unique_type = df_type["type"].unique()
print(unique_type)

unique_subtype = df_type["subtype"].unique()
print(unique_subtype)
```

There are four "types" present in the dataset (i.e. accident, hazard, jam, and road closed), and all four have a missing value "subtype." The Hazard type has enough information to consider that it could have sub-subtypes.

* Accident
  + Major
  + Minor

* Hazard
  + On road
    - Car stopped
    - Construction
    - Emergency vehicle
    - Ice
    - Lane closed
    - Object
    - Pot hole
    - Road kill
    - Traffic light fault
  + On shoulder
    - Animals
    - Car stopped
    - Missing sign
  + Weather
    - Flood
    - Fog
    - Hail
    - Heavy snow

* Jam
  + Heavy traffic
  + Light traffic
  + Moderate traffic
  + Stand still traffic

* Road Closed
  + Construction
  + Event
  + Hazard

I think we should keep the NA subtype because there is a significant number of entries in the dataset with this subtype, and removing them could change the composition of our data and affect the results of our analysis/dashboard.

```{python}
# Reclassify missing values
df_type["subtype"] = df_type["subtype"].fillna("Unclassified")
```

4. 

a. 
```{python}
# Create new df with initial columns
df_crosswalk = pd.DataFrame()
df_crosswalk["type"] = df_type["type"]
df_crosswalk["subtype"] = df_type["subtype"]
df_crosswalk["updated_type"] = ""
df_crosswalk["updated_subtype"] = ""
df_crosswalk["updated_subsubtype"] = ""
```

b. 

```{python}
# Fill in updated type
df_crosswalk["updated_type"] = df_crosswalk["type"].str.capitalize()

# Define a function to update subtype


def update_subtype(row):
    # Check if unclassified
    if row == "Unclassified":
        return "Unclassified"

    # Check if hazard
    if "HAZARD" in row:

        # Create list of Hazard subtypes
        hazard_subtypes = ["ON_ROAD", "ON_SHOULDER", "WEATHER"]

        # Iterate over list
        for index in range(0, 3):
            if hazard_subtypes[index] in row:
                return hazard_subtypes[index]

    # Create list of remaining alert types
    alert_types = ["ACCIDENT", "JAM", "ROAD_CLOSED"]

    # Iterate over list
    for index in range(0, 3):
        length = len(alert_types[index])

        if alert_types[index] in row:
            return row[(length + 1):]


# Fill in updated subtype
df_crosswalk["updated_subtype"] = df_crosswalk["subtype"].apply(update_subtype)
df_crosswalk["updated_subtype"] = df_crosswalk["updated_subtype"].str.capitalize()

# Define a function to update subsubtype


def update_subsubtype(row):
    # Check if hazard
    if "HAZARD" in row:

        # Create list of Hazard subtypes
        hazard_subtypes = ["ON_ROAD_", "ON_SHOULDER_", "WEATHER_"]

        # Iterate over list
        for index in range(0, 3):
            length = len(hazard_subtypes[index])

            if hazard_subtypes[index] in row:
                return row[(length + 7):]

    else:
        return np.nan


# Fill in updated subsubtype
df_crosswalk["updated_subsubtype"] = df_crosswalk["subtype"].apply(
    update_subsubtype)
df_crosswalk["updated_subsubtype"] = df_crosswalk["updated_subsubtype"].str.capitalize()
```

c. 

```{python}
# Reclassify missing subtypes in original dataset
df_waze["subtype"] = df_waze["subtype"].fillna("Unclassified")

# Merge crosswalk with original df
df_merged = df_waze.merge(df_crosswalk, on=["type", "subtype"])

# Count number of rows for Accident - Unclassified
updated_type_subtype_count = df_merged.value_counts(
    ["updated_type", "updated_subtype"])
updated_type_subtype_count.get(("Accident", "Unclassified"))
```

There are 24,359 rows tagged as Accident - Unclassified.

d. 

```{python}
# Compare "type" values for crosswalk and merged dfs
crosswalk_types = set(df_crosswalk["type"].unique())
merged_types = set(df_merged["type"].unique())

crosswalk_types == merged_types
```

```{python}
# Compare "subtype" values for crosswalk and merged dfs
crosswalk_subtypes = set(df_crosswalk["subtype"].unique())
merged_subtypes = set(df_merged["subtype"].unique())

crosswalk_subtypes == merged_subtypes
```


# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
# Extract coordinates from geo data
coordinate_pattern = r"([-+]?\d+\.\d+)\s+([-+]?\d+\.\d+)"
df_merged[["longitude", "latitude"]
          ] = df_merged["geo"].str.extract(coordinate_pattern)

# Convert coordinates to float
df_merged["longitude"] = df_merged["longitude"].astype(float)
df_merged["latitude"] = df_merged["latitude"].astype(float)
```

ChatGPT prompt: in python, create a regular expression that extracts coordinates from a geo variable in a dataframe that holds coordinates data in the form POINT(-87.676685 41.929692)

b. 
```{python}
# Bin coordinates by rounding
df_merged["longitude"] = df_merged["longitude"].round(2)
df_merged["latitude"] = df_merged["latitude"].round(2)

# Count observations per coordinate pair
df_coord_count = df_merged.groupby(
    ["longitude", "latitude"]).size().reset_index()
df_coord_count.sort_values(by=0, ascending=False, inplace=True)
df_coord_count.head(1)
```

The binned coordinates (-87.65, 41.88) has the greatest number of observations in the dataset with 21,325.

c. 

```{python}
# Aggregate binned counts based on type, subtype, and coordinates
df_coord_count = df_merged.groupby(
    ["updated_type", "updated_subtype", "longitude", "latitude"]).size().reset_index()
df_coord_count = df_coord_count.rename(columns={0: "count"})

# Aggregate by type and subtype and get top 10 largest rows by count
df_coord_count_top10 = df_coord_count.groupby(
    ["updated_type", "updated_subtype"], group_keys=False).apply(lambda x: x.nlargest(10, "count"))

# Save to csv
folder = "top_alerts_map"
filepath = os.path.join(base, folder, "top_alerts_map.csv")
df_coord_count_top10.to_csv(filepath, index=False)

# Get number of rows
len(df_coord_count_top10)
```

The dataframe has 155 rows. Aggregation was first done based on type, subtype, latitude, and longitude to get the number of observations for each unique combination. Then we further aggregate the data to get the top 10 rows for each unique combination of type and subtype based on count.

2. 
```{python}
# Filter data for "Jam - Heavy Traffic" alerts
df_jam_heavy = df_coord_count_top10.loc[
    (df_coord_count_top10["updated_type"] == "Jam") & (df_coord_count_top10["updated_subtype"] == "Heavy_traffic")]

# Set max and min values
constant = 0.020
x_min = min(df_jam_heavy["longitude"]) - constant
x_max = max(df_jam_heavy["longitude"]) + constant
y_min = min(df_jam_heavy["latitude"]) - constant
y_max = max(df_jam_heavy["latitude"]) + constant

# Create scatter plot
scatter_plot = alt.Chart(df_jam_heavy).mark_circle().encode(
    x=alt.X("longitude:Q", scale=alt.Scale(domain=[x_min, x_max])),
    y=alt.Y("latitude:Q", scale=alt.Scale(domain=[y_min, y_max])),
    size=alt.Size("count:Q")
)

scatter_plot
```

3. 
    
a. 

```{python}

```

b. 
```{python}
# MODIFY ACCORDINGLY
file_path = "./top_alerts_map/chicago-boundaries.geojson"
#----

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])

```

4. 

```{python}

```

5. 

a. 

```{python}

```

b. 
```{python}

```

c. 
```{python}

```

d. 
```{python}

```

e. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 


    
b. 
```{python}

```

c.

```{python}

```
    

2.

a. 



b. 


c. 


# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

b. 

```{python}

```

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
